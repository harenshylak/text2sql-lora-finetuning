{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7aced0e-3be2-4aa8-9f38-c5d5b5652349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T21:18:51.220268Z",
     "iopub.status.busy": "2025-10-14T21:18:51.219972Z",
     "iopub.status.idle": "2025-10-14T21:18:51.503901Z",
     "shell.execute_reply": "2025-10-14T21:18:51.503208Z",
     "shell.execute_reply.started": "2025-10-14T21:18:51.220242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user\n",
      "/home/sagemaker-user\n"
     ]
    }
   ],
   "source": [
    "!echo $HOME\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9404d00c-14c1-4a7d-87f7-2aafe50db700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T21:20:35.317513Z",
     "iopub.status.busy": "2025-10-14T21:20:35.317307Z",
     "iopub.status.idle": "2025-10-14T21:20:36.819868Z",
     "shell.execute_reply": "2025-10-14T21:20:36.819337Z",
     "shell.execute_reply.started": "2025-10-14T21:20:35.317494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin  include  lib  lib64  pyvenv.cfg\n"
     ]
    }
   ],
   "source": [
    "!python -m venv /home/sagemaker-user/t2sql-env\n",
    "!ls /home/sagemaker-user/t2sql-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d0bd42-8a25-4bbb-be16-cbe2f66ce669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T21:41:10.181988Z",
     "iopub.status.busy": "2025-10-14T21:41:10.181762Z",
     "iopub.status.idle": "2025-10-14T21:41:12.500590Z",
     "shell.execute_reply": "2025-10-14T21:41:12.499897Z",
     "shell.execute_reply.started": "2025-10-14T21:41:10.181972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: multiprocess 0.70.16\n",
      "Uninstalling multiprocess-0.70.16:\n",
      "  Successfully uninstalled multiprocess-0.70.16\n",
      "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
      "Python: 3.12.9 | packaged by conda-forge | (main, Feb 14 2025, 08:00:06) [GCC 13.3.0]\n",
      "transformers: 4.44.2\n",
      "datasets: 4.2.0\n",
      "peft: 0.17.1\n",
      "accelerate: 1.10.1\n",
      "pyarrow: 21.0.0\n",
      "multiprocess: 0.70.16\n"
     ]
    }
   ],
   "source": [
    "# 0) make sure we're using the venv's pip\n",
    "!\"$HOME/t2sql-env/bin/python\" -m pip install -q --upgrade pip\n",
    "\n",
    "# 1) if you previously installed a conflicting multiprocess, remove it\n",
    "!\"$HOME/t2sql-env/bin/pip\" uninstall -y multiprocess || true\n",
    "\n",
    "# 2) install a known-good, minimal stack (no multiprocess pin; datasets pulls a compatible one)\n",
    "!\"$HOME/t2sql-env/bin/pip\" install -q \\\n",
    "  \"transformers==4.44.2\" \\\n",
    "  \"datasets==2.19.0\" \\\n",
    "  \"peft==0.12.0\" \\\n",
    "  \"accelerate==0.33.0\" \\\n",
    "  sentencepiece \\\n",
    "  \"pyarrow==19.0.0\"\n",
    "\n",
    "# 3) confirm versions actually installed inside the venv\n",
    "!\"$HOME/t2sql-env/bin/python\" - <<'PY'\n",
    "import transformers, datasets, peft, accelerate, pyarrow, sys\n",
    "import multiprocess\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"pyarrow:\", pyarrow.__version__)\n",
    "print(\"multiprocess:\", multiprocess.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "449dd567-dea4-42db-ba6d-4e0f6fe5bf11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T21:41:12.501691Z",
     "iopub.status.busy": "2025-10-14T21:41:12.501497Z",
     "iopub.status.idle": "2025-10-14T21:41:13.004812Z",
     "shell.execute_reply": "2025-10-14T21:41:13.004228Z",
     "shell.execute_reply.started": "2025-10-14T21:41:12.501672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 29M\n",
      "-rw-r--r-- 1 sagemaker-user users 3.5M Oct 14 21:06 dev.json\n",
      "-rw-r--r-- 1 sagemaker-user users 792K Oct 14 21:06 tables.json\n",
      "-rw-r--r-- 1 sagemaker-user users  24M Oct 14 21:06 train_spider.json\n"
     ]
    }
   ],
   "source": [
    "# put data in place (if not already)\n",
    "!mkdir -p \"$HOME/text2sql_data\"\n",
    "!mv -f train_spider.json dev.json tables.json \"$HOME/text2sql_data\"/ 2>/dev/null || true\n",
    "!ls -lh \"$HOME/text2sql_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f24ed1-80a6-4d04-84d4-29952d64f540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:00:28.091688Z",
     "iopub.status.busy": "2025-10-14T22:00:28.091462Z",
     "iopub.status.idle": "2025-10-14T22:00:28.109771Z",
     "shell.execute_reply": "2025-10-14T22:00:28.109112Z",
     "shell.execute_reply.started": "2025-10-14T22:00:28.091672Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ~/train_text2sql_lora.py <<'PY'\n",
    "import os, json, argparse, re, csv\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# ---------- utils\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def normalize_sql(sql: str) -> str:\n",
    "    s = re.sub(r\"\\\\s+\", \" \", (sql or \"\")).strip().lower()\n",
    "    s = re.sub(r\"(<=|>=|=|<|>)\", lambda m: f\" {m.group(0)} \", s)\n",
    "    return re.sub(r\"\\\\s+\", \" \", s).strip().rstrip(\";\")\n",
    "\n",
    "# ---------- schema formatting (compact, capped)\n",
    "\n",
    "def build_schema_texts(tables_json_path: str, keep_db_ids: set=None, cap_cols=6, cap_fks=6) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns {db_id: compact_schema_text} with capped columns & FKs to keep prompts short.\n",
    "    \"\"\"\n",
    "    tables = load_json(tables_json_path)\n",
    "    out = {}\n",
    "    for db in tables:\n",
    "        db_id = db[\"db_id\"]\n",
    "        if keep_db_ids and db_id not in keep_db_ids:\n",
    "            continue\n",
    "\n",
    "        names = db[\"table_names_original\"]          # list[str]\n",
    "        cols  = db[\"column_names_original\"]         # list[[t_idx, col_name]]\n",
    "        pks   = set(db[\"primary_keys\"])\n",
    "        fks   = db[\"foreign_keys\"]\n",
    "\n",
    "        per_table = {i: [] for i in range(len(names))}\n",
    "        for idx, (t_idx, c_name) in enumerate(cols):\n",
    "            if t_idx == -1:\n",
    "                continue\n",
    "            tag = \" PK\" if idx in pks else \"\"\n",
    "            per_table[t_idx].append(f\"{c_name}{tag}\")\n",
    "\n",
    "        fk_lines = []\n",
    "        for child, parent in fks:\n",
    "            ct, cc = cols[child]\n",
    "            pt, pc = cols[parent]\n",
    "            if ct == -1 or pt == -1:\n",
    "                continue\n",
    "            fk_lines.append(f\"{names[ct]}.{cc}->{names[pt]}.{pc}\")\n",
    "\n",
    "        lines = [f\"DB: {db_id}\", \"Tables:\"]\n",
    "        for i, t in enumerate(names):\n",
    "            c = \", \".join(per_table[i][:cap_cols])\n",
    "            if len(per_table[i]) > cap_cols:\n",
    "                c += \", ...\"\n",
    "            lines.append(f\"- {t}({c})\")\n",
    "\n",
    "        if fk_lines:\n",
    "            if len(fk_lines) > cap_fks:\n",
    "                fk_show = \"; \".join(fk_lines[:cap_fks]) + \"; ...\"\n",
    "            else:\n",
    "                fk_show = \"; \".join(fk_lines)\n",
    "            lines.append(\"FKs: \" + fk_show)\n",
    "\n",
    "        out[db_id] = \"\\\\n\".join(lines)\n",
    "    return out\n",
    "\n",
    "# ---------- dataset construction (whitelist fields)\n",
    "\n",
    "def make_hf_datasets(root: str, train_json: str, val_json: str, schema_texts: Dict[str,str]):\n",
    "    train_raw = load_json(os.path.join(root, train_json))\n",
    "    val_raw   = load_json(os.path.join(root, val_json))\n",
    "\n",
    "    def to_record(x):\n",
    "        q = str(x.get(\"question\",\"\")).strip()\n",
    "        sql = str(x.get(\"query\",\"\")).strip()\n",
    "        db = str(x.get(\"db_id\",\"\")).strip()\n",
    "        schema = schema_texts.get(db, f\"DB: {db}\\\\nTables: (missing)\")\n",
    "        return {\n",
    "            \"input_text\": f\"translate to sql: {q}\\\\n{schema}\",\n",
    "            \"labels\": sql,\n",
    "            \"db_id\": db,\n",
    "            \"question\": q,\n",
    "        }\n",
    "\n",
    "    train_ds = Dataset.from_list([to_record(z) for z in train_raw])\n",
    "    val_ds   = Dataset.from_list([to_record(z) for z in val_raw])\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def tokenize_datasets(train_ds: Dataset, val_ds: Dataset, tok, max_src=512, max_tgt=160):\n",
    "    def _prep(batch):\n",
    "        mi = tok(batch[\"input_text\"], max_length=max_src, truncation=True)\n",
    "        with tok.as_target_tokenizer():\n",
    "            labels = tok(batch[\"labels\"], max_length=max_tgt, truncation=True)\n",
    "        mi[\"labels\"] = labels[\"input_ids\"]\n",
    "        return mi\n",
    "    train_tok = train_ds.map(_prep, batched=True, remove_columns=train_ds.column_names)\n",
    "    val_tok   = val_ds.map(_prep,   batched=True, remove_columns=val_ds.column_names)\n",
    "    return train_tok, val_tok\n",
    "\n",
    "# ---------- EM evaluation with truncation + safe decoding\n",
    "\n",
    "def exact_match_eval(model, tok, val_ds, out_csv=None, max_gen_len=160, num_beams=4, max_src_len=512):\n",
    "    device = next(model.parameters()).device\n",
    "    preds, refs, rows = [], [], []\n",
    "\n",
    "    for ex in val_ds:\n",
    "        prompt = ex[\"input_text\"]\n",
    "        gold   = ex[\"labels\"]\n",
    "        db_id  = ex.get(\"db_id\", \"\")\n",
    "        schema = ex[\"input_text\"].split(\"\\\\n\",1)[1] if \"\\\\n\" in ex[\"input_text\"] else \"\"\n",
    "\n",
    "        enc = tok(prompt, return_tensors=\"pt\", max_length=max_src_len, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_gen_len,\n",
    "                num_beams=num_beams,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=0.8,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tok.eos_token_id,\n",
    "            )\n",
    "        pred = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        preds.append(normalize_sql(pred))\n",
    "        refs.append(normalize_sql(gold))\n",
    "        rows.append({\"db_id\": db_id, \"question\": ex.get(\"question\",\"\"), \"pred\": pred, \"gold\": gold, \"schema\": schema})\n",
    "\n",
    "    em = (sum(p == r for p, r in zip(preds, refs)) / len(refs)) if refs else 0.0\n",
    "\n",
    "    if out_csv:\n",
    "        os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "        with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=[\"db_id\",\"question\",\"pred\",\"gold\",\"schema\"])\n",
    "            w.writeheader()\n",
    "            w.writerows(rows)\n",
    "\n",
    "    return em\n",
    "\n",
    "# ---------- main\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data_root\", required=True, help=\"Folder with train/val/tables json files\")\n",
    "    ap.add_argument(\"--train_json\", default=\"train_spider.json\")\n",
    "    ap.add_argument(\"--val_json\",   default=\"dev.json\")\n",
    "    ap.add_argument(\"--tables_json\", default=\"tables.json\")\n",
    "    ap.add_argument(\"--model_name\", default=\"google/flan-t5-base\")\n",
    "    ap.add_argument(\"--output_dir\", default=\"nl2sql-lora-out\")\n",
    "    ap.add_argument(\"--epochs\", type=int, default=3)\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    ap.add_argument(\"--grad_accum\", type=int, default=2)\n",
    "    ap.add_argument(\"--lr\", type=float, default=5e-4)\n",
    "    ap.add_argument(\"--max_src_len\", type=int, default=512)\n",
    "    ap.add_argument(\"--max_tgt_len\", type=int, default=160)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    train_path = os.path.join(args.data_root, args.train_json)\n",
    "    val_path   = os.path.join(args.data_root, args.val_json)\n",
    "    tables_path= os.path.join(args.data_root, args.tables_json)\n",
    "\n",
    "    # Build compact schema texts for only DBs we need\n",
    "    train_items = load_json(train_path)\n",
    "    val_items   = load_json(val_path)\n",
    "    keep_db_ids = set([x[\"db_id\"] for x in train_items + val_items])\n",
    "    schema_texts = build_schema_texts(tables_path, keep_db_ids=keep_db_ids, cap_cols=6, cap_fks=6)\n",
    "\n",
    "    # Datasets & tokenizer\n",
    "    train_ds, val_ds = make_hf_datasets(args.data_root, args.train_json, args.val_json, schema_texts)\n",
    "    tok = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    train_tok, val_tok = tokenize_datasets(train_ds, val_ds, tok, args.max_src_len, args.max_tgt_len)\n",
    "\n",
    "    # Model + LoRA (broader targets), no 8-bit to keep it simple/stable\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "        target_modules=[\"q\",\"k\",\"v\",\"o\",\"wi_0\",\"wi_1\",\"wo\"],  # attention + FFN\n",
    "        task_type=\"SEQ_2_SEQ_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Trainer\n",
    "    collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model)\n",
    "    targs = Seq2SeqTrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.grad_accum,\n",
    "        learning_rate=args.lr,\n",
    "        num_train_epochs=args.epochs,\n",
    "        logging_steps=50,\n",
    "\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "\n",
    "        # Stability while debugging\n",
    "        fp16=False, bf16=False,\n",
    "        label_smoothing_factor=0.1,\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=targs,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tok\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save LoRA adapters + tokenizer\n",
    "    adapters_dir = os.path.join(args.output_dir, \"lora_adapters\")\n",
    "    os.makedirs(adapters_dir, exist_ok=True)\n",
    "    model.save_pretrained(adapters_dir)\n",
    "    tok.save_pretrained(adapters_dir)\n",
    "\n",
    "    # Quick EM + judge CSV\n",
    "    em = exact_match_eval(\n",
    "        model, tok, val_ds,\n",
    "        out_csv=os.path.join(args.output_dir, \"judge_input.csv\"),\n",
    "        max_gen_len=args.max_tgt_len,\n",
    "        num_beams=4,\n",
    "        max_src_len=args.max_src_len\n",
    "    )\n",
    "    print(f\"[VAL] Exact Match: {em:.4f}\")\n",
    "    print(f\"Saved adapters to: {adapters_dir}\")\n",
    "    print(f\"Saved judge CSV to: {os.path.join(args.output_dir, 'judge_input.csv')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea3f10a-e33a-4b56-8ac5-c0b8a1a5c33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:00:43.018712Z",
     "iopub.status.busy": "2025-10-14T22:00:43.018478Z",
     "iopub.status.idle": "2025-10-14T22:37:05.329400Z",
     "shell.execute_reply": "2025-10-14T22:37:05.328692Z",
     "shell.execute_reply.started": "2025-10-14T22:00:43.018695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 2.54kB [00:00, 21.7MB/s]\n",
      "spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:00<00:00, 4.73MB/s]\n",
      "tokenizer.json: 2.42MB [00:00, 199MB/s]\n",
      "special_tokens_map.json: 2.20kB [00:00, 29.3MB/s]\n",
      "/home/sagemaker-user/t2sql-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map:   0%|                                      | 0/7000 [00:00<?, ? examples/s]/home/sagemaker-user/t2sql-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7000/7000 [00:01<00:00, 4356.42 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1034/1034 [00:00<00:00, 2730.69 examples/s]\n",
      "config.json: 1.40kB [00:00, 13.6MB/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 990M/990M [00:02<00:00, 387MB/s]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:00<00:00, 2.03MB/s]\n",
      "trainable params: 3,391,488 || all params: 250,969,344 || trainable%: 1.3514\n",
      "/home/sagemaker-user/t2sql-env/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "{'loss': 3.0407, 'grad_norm': 0.41954943537712097, 'learning_rate': 0.00048093058733791, 'epoch': 0.11}\n",
      "{'loss': 2.3315, 'grad_norm': 0.29505568742752075, 'learning_rate': 0.00046186117467582, 'epoch': 0.23}\n",
      "{'loss': 2.1733, 'grad_norm': 0.3367687463760376, 'learning_rate': 0.00044279176201373, 'epoch': 0.34}\n",
      "{'loss': 2.0987, 'grad_norm': 0.4911755621433258, 'learning_rate': 0.00042372234935164, 'epoch': 0.46}\n",
      "{'loss': 2.0436, 'grad_norm': 0.30722829699516296, 'learning_rate': 0.00040465293668954995, 'epoch': 0.57}\n",
      "{'loss': 2.0106, 'grad_norm': 0.37235936522483826, 'learning_rate': 0.00038558352402745993, 'epoch': 0.69}\n",
      "{'loss': 1.9893, 'grad_norm': 0.3042076826095581, 'learning_rate': 0.00036651411136536996, 'epoch': 0.8}\n",
      "{'loss': 1.955, 'grad_norm': 0.2883242070674896, 'learning_rate': 0.00034744469870327994, 'epoch': 0.91}\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 437/1311 [06:01<12:05,  1.20it/s]\n",
      "  0%|                                                   | 0/130 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|â–‰                                          | 3/130 [00:00<00:05, 22.22it/s]\u001b[A\n",
      "  5%|â–ˆâ–‰                                         | 6/130 [00:00<00:07, 16.05it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–‹                                        | 8/130 [00:00<00:07, 15.30it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 10/130 [00:00<00:08, 14.51it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 12/130 [00:00<00:08, 14.38it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 14/130 [00:00<00:08, 13.59it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 16/130 [00:01<00:08, 13.21it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 18/130 [00:01<00:08, 12.87it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 20/130 [00:01<00:08, 12.52it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 22/130 [00:01<00:09, 11.98it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 24/130 [00:01<00:09, 11.52it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 26/130 [00:01<00:08, 12.88it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 28/130 [00:02<00:07, 13.93it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 30/130 [00:02<00:06, 14.77it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 32/130 [00:02<00:06, 15.14it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 34/130 [00:02<00:06, 15.37it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 36/130 [00:02<00:06, 14.99it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 38/130 [00:02<00:06, 14.39it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 40/130 [00:02<00:06, 13.58it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 42/130 [00:03<00:06, 13.26it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 44/130 [00:03<00:06, 12.95it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 46/130 [00:03<00:06, 12.52it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 48/130 [00:03<00:06, 12.25it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 50/130 [00:03<00:06, 13.08it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 52/130 [00:03<00:05, 14.01it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 54/130 [00:03<00:05, 14.53it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 56/130 [00:04<00:05, 14.40it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 58/130 [00:04<00:04, 14.43it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 60/130 [00:04<00:04, 14.83it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 62/130 [00:04<00:04, 15.17it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 64/130 [00:04<00:05, 12.47it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 66/130 [00:05<00:06,  9.20it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 68/130 [00:05<00:07,  7.76it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 69/130 [00:05<00:08,  7.25it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 70/130 [00:05<00:08,  6.85it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 71/130 [00:05<00:08,  6.60it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 72/130 [00:06<00:09,  6.34it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 73/130 [00:06<00:09,  6.02it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 74/130 [00:06<00:09,  5.86it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 76/130 [00:06<00:06,  7.85it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 78/130 [00:06<00:05,  9.77it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 80/130 [00:06<00:04, 11.27it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 82/130 [00:06<00:03, 12.17it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 84/130 [00:07<00:03, 13.47it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 86/130 [00:07<00:03, 14.52it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 88/130 [00:07<00:02, 15.19it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 90/130 [00:07<00:02, 14.79it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 92/130 [00:07<00:02, 15.61it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 94/130 [00:07<00:02, 16.15it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 96/130 [00:07<00:02, 15.75it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 98/130 [00:07<00:01, 16.16it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 100/130 [00:08<00:01, 16.14it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 102/130 [00:08<00:01, 16.56it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 104/130 [00:08<00:01, 16.38it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 106/130 [00:08<00:01, 15.77it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 108/130 [00:08<00:01, 15.24it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 110/130 [00:08<00:01, 15.43it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 112/130 [00:08<00:01, 15.99it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 114/130 [00:08<00:00, 16.33it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 116/130 [00:09<00:01, 13.24it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/130 [00:09<00:01, 11.00it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 120/130 [00:09<00:01,  9.97it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/130 [00:09<00:00,  9.44it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 124/130 [00:10<00:00,  9.27it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/130 [00:10<00:00,  9.26it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 127/130 [00:10<00:00, 10.60it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.9173451662063599, 'eval_runtime': 10.6623, 'eval_samples_per_second': 96.977, 'eval_steps_per_second': 12.193, 'epoch': 1.0}\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 437/1311 [06:12<12:05,  1.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 11.51it/s]\u001b[A\n",
      "{'loss': 1.9344, 'grad_norm': 0.3283551037311554, 'learning_rate': 0.0003283752860411899, 'epoch': 1.03}\n",
      "{'loss': 1.9286, 'grad_norm': 0.3750941753387451, 'learning_rate': 0.00030930587337909995, 'epoch': 1.14}\n",
      "{'loss': 1.8928, 'grad_norm': 0.34804314374923706, 'learning_rate': 0.00029023646071700993, 'epoch': 1.26}\n",
      "{'loss': 1.8893, 'grad_norm': 0.339126855134964, 'learning_rate': 0.0002711670480549199, 'epoch': 1.37}\n",
      "{'loss': 1.8818, 'grad_norm': 0.4918936789035797, 'learning_rate': 0.00025209763539282994, 'epoch': 1.49}\n",
      "{'loss': 1.8712, 'grad_norm': 0.3321043848991394, 'learning_rate': 0.0002330282227307399, 'epoch': 1.6}\n",
      "{'loss': 1.8604, 'grad_norm': 0.3004568815231323, 'learning_rate': 0.0002139588100686499, 'epoch': 1.71}\n",
      "{'loss': 1.8564, 'grad_norm': 0.3665751814842224, 'learning_rate': 0.0001948893974065599, 'epoch': 1.83}\n",
      "{'loss': 1.8439, 'grad_norm': 0.2854655981063843, 'learning_rate': 0.00017581998474446985, 'epoch': 1.94}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 875/1311 [12:14<05:43,  1.27it/s]\n",
      "  0%|                                                   | 0/130 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|â–‰                                          | 3/130 [00:00<00:05, 22.27it/s]\u001b[A\n",
      "  5%|â–ˆâ–‰                                         | 6/130 [00:00<00:07, 16.02it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–‹                                        | 8/130 [00:00<00:07, 15.45it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 10/130 [00:00<00:08, 14.60it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 12/130 [00:00<00:08, 14.47it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 14/130 [00:00<00:08, 13.64it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 16/130 [00:01<00:08, 13.30it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 18/130 [00:01<00:08, 12.93it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 20/130 [00:01<00:08, 12.56it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 22/130 [00:01<00:08, 12.05it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 24/130 [00:01<00:09, 11.53it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 26/130 [00:01<00:08, 12.82it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 28/130 [00:02<00:07, 13.81it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 30/130 [00:02<00:06, 14.59it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 32/130 [00:02<00:06, 14.92it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 34/130 [00:02<00:06, 15.15it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 36/130 [00:02<00:06, 14.83it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 38/130 [00:02<00:06, 14.29it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 40/130 [00:02<00:06, 13.48it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 42/130 [00:03<00:06, 13.15it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 44/130 [00:03<00:06, 12.89it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 46/130 [00:03<00:06, 12.50it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 48/130 [00:03<00:06, 12.20it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 50/130 [00:03<00:06, 13.10it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 52/130 [00:03<00:05, 14.09it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 54/130 [00:03<00:05, 14.66it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 56/130 [00:04<00:05, 14.54it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 58/130 [00:04<00:04, 14.53it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 60/130 [00:04<00:04, 14.94it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 62/130 [00:04<00:04, 15.20it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 64/130 [00:04<00:05, 12.50it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 66/130 [00:05<00:06,  9.20it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 68/130 [00:05<00:07,  7.76it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 69/130 [00:05<00:08,  7.24it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 70/130 [00:05<00:08,  6.84it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 71/130 [00:05<00:08,  6.60it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 72/130 [00:06<00:09,  6.34it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 73/130 [00:06<00:09,  6.03it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 74/130 [00:06<00:09,  5.86it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 76/130 [00:06<00:06,  7.89it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 78/130 [00:06<00:05,  9.81it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 80/130 [00:06<00:04, 11.32it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 82/130 [00:06<00:03, 12.24it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 84/130 [00:07<00:03, 13.59it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 86/130 [00:07<00:03, 14.65it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 88/130 [00:07<00:02, 15.19it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 90/130 [00:07<00:02, 14.83it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 92/130 [00:07<00:02, 15.50it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 94/130 [00:07<00:02, 15.94it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 96/130 [00:07<00:02, 15.52it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 98/130 [00:07<00:02, 15.82it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 100/130 [00:08<00:01, 15.87it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 102/130 [00:08<00:01, 16.30it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 104/130 [00:08<00:01, 16.20it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 106/130 [00:08<00:01, 15.64it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 108/130 [00:08<00:01, 15.14it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 110/130 [00:08<00:01, 15.29it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 112/130 [00:08<00:01, 15.87it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 114/130 [00:08<00:00, 16.25it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 116/130 [00:09<00:01, 13.20it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/130 [00:09<00:01, 11.01it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 120/130 [00:09<00:01,  9.98it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/130 [00:09<00:00,  9.46it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 124/130 [00:10<00:00,  9.26it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/130 [00:10<00:00,  9.26it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 127/130 [00:10<00:00, 10.59it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8631248474121094, 'eval_runtime': 10.6803, 'eval_samples_per_second': 96.814, 'eval_steps_per_second': 12.172, 'epoch': 2.0}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 875/1311 [12:25<05:43,  1.27it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 11.46it/s]\u001b[A\n",
      "{'loss': 1.8325, 'grad_norm': 0.3752608895301819, 'learning_rate': 0.00015675057208237986, 'epoch': 2.06}\n",
      "{'loss': 1.8279, 'grad_norm': 0.4797810912132263, 'learning_rate': 0.00013768115942028986, 'epoch': 2.17}\n",
      "{'loss': 1.8288, 'grad_norm': 0.3705190420150757, 'learning_rate': 0.00011861174675819985, 'epoch': 2.29}\n",
      "{'loss': 1.8302, 'grad_norm': 0.33759188652038574, 'learning_rate': 9.954233409610984e-05, 'epoch': 2.4}\n",
      "{'loss': 1.8149, 'grad_norm': 0.41688272356987, 'learning_rate': 8.047292143401984e-05, 'epoch': 2.51}\n",
      "{'loss': 1.8213, 'grad_norm': 0.40463536977767944, 'learning_rate': 6.140350877192983e-05, 'epoch': 2.63}\n",
      "{'loss': 1.8063, 'grad_norm': 0.37954920530319214, 'learning_rate': 4.233409610983982e-05, 'epoch': 2.74}\n",
      "{'loss': 1.819, 'grad_norm': 0.4214041233062744, 'learning_rate': 2.326468344774981e-05, 'epoch': 2.86}\n",
      "{'loss': 1.8071, 'grad_norm': 0.44168993830680847, 'learning_rate': 4.195270785659802e-06, 'epoch': 2.97}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1311/1311 [18:26<00:00,  1.21it/s]\n",
      "  0%|                                                   | 0/130 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|â–‰                                          | 3/130 [00:00<00:05, 22.28it/s]\u001b[A\n",
      "  5%|â–ˆâ–‰                                         | 6/130 [00:00<00:07, 16.04it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–‹                                        | 8/130 [00:00<00:07, 15.46it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 10/130 [00:00<00:08, 14.51it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 12/130 [00:00<00:08, 14.39it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 14/130 [00:00<00:08, 13.60it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 16/130 [00:01<00:08, 13.28it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 18/130 [00:01<00:08, 12.94it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 20/130 [00:01<00:08, 12.56it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 22/130 [00:01<00:08, 12.03it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 24/130 [00:01<00:09, 11.51it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 26/130 [00:01<00:08, 12.83it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 28/130 [00:02<00:07, 13.87it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 30/130 [00:02<00:06, 14.74it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 32/130 [00:02<00:06, 15.11it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 34/130 [00:02<00:06, 15.36it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 36/130 [00:02<00:06, 14.98it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 38/130 [00:02<00:06, 14.39it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 40/130 [00:02<00:06, 13.58it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 42/130 [00:03<00:06, 13.25it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 44/130 [00:03<00:06, 12.93it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 46/130 [00:03<00:06, 12.52it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 48/130 [00:03<00:06, 12.24it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 50/130 [00:03<00:06, 13.12it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 52/130 [00:03<00:05, 14.12it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 54/130 [00:03<00:05, 14.73it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 56/130 [00:04<00:05, 14.53it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 58/130 [00:04<00:04, 14.49it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 60/130 [00:04<00:04, 14.90it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 62/130 [00:04<00:04, 15.18it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 64/130 [00:04<00:05, 12.48it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 66/130 [00:05<00:06,  9.21it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 68/130 [00:05<00:07,  7.77it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 69/130 [00:05<00:08,  7.25it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 70/130 [00:05<00:08,  6.85it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 71/130 [00:05<00:08,  6.60it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 72/130 [00:06<00:09,  6.34it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 73/130 [00:06<00:09,  6.03it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 74/130 [00:06<00:09,  5.86it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 76/130 [00:06<00:06,  7.86it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 78/130 [00:06<00:05,  9.85it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 80/130 [00:06<00:04, 11.31it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 82/130 [00:06<00:03, 12.19it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 84/130 [00:07<00:03, 13.45it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 86/130 [00:07<00:03, 14.47it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 88/130 [00:07<00:02, 15.08it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 90/130 [00:07<00:02, 14.89it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 92/130 [00:07<00:02, 15.69it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 94/130 [00:07<00:02, 16.23it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 96/130 [00:07<00:02, 15.79it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 98/130 [00:07<00:01, 16.19it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 100/130 [00:08<00:01, 16.17it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 102/130 [00:08<00:01, 16.51it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 104/130 [00:08<00:01, 16.36it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 106/130 [00:08<00:01, 15.74it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 108/130 [00:08<00:01, 15.22it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 110/130 [00:08<00:01, 15.34it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 112/130 [00:08<00:01, 15.94it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 114/130 [00:08<00:00, 16.21it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 116/130 [00:09<00:01, 13.19it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/130 [00:09<00:01, 11.00it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 120/130 [00:09<00:01,  9.98it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/130 [00:09<00:00,  9.44it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 124/130 [00:10<00:00,  9.25it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/130 [00:10<00:00,  9.25it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 127/130 [00:10<00:00, 10.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.857425570487976, 'eval_runtime': 10.6511, 'eval_samples_per_second': 97.08, 'eval_steps_per_second': 12.205, 'epoch': 3.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1311/1311 [18:37<00:00,  1.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 11.55it/s]\u001b[A\n",
      "{'train_runtime': 1117.65, 'train_samples_per_second': 18.789, 'train_steps_per_second': 1.173, 'train_loss': 1.9599793839327349, 'epoch': 3.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1311/1311 [18:37<00:00,  1.17it/s]\n",
      "[VAL] Exact Match: 0.1093\n",
      "Saved adapters to: /home/sagemaker-user/text2sql_outputs/lora_adapters\n",
      "Saved judge CSV to: /home/sagemaker-user/text2sql_outputs/judge_input.csv\n"
     ]
    }
   ],
   "source": [
    "!\"$HOME/t2sql-env/bin/python\" \"$HOME/train_text2sql_lora.py\" \\\n",
    "  --data_root \"$HOME/text2sql_data\" \\\n",
    "  --train_json train_spider.json \\\n",
    "  --val_json dev.json \\\n",
    "  --tables_json tables.json \\\n",
    "  --model_name google/flan-t5-base \\\n",
    "  --output_dir \"$HOME/text2sql_outputs\" \\\n",
    "  --epochs 3 \\\n",
    "  --batch_size 8 \\\n",
    "  --grad_accum 2 \\\n",
    "  --lr 5e-4 \\\n",
    "  --max_src_len 512 \\\n",
    "  --max_tgt_len 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1621873-b833-45bf-9ae3-b42d27a24c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:37:08.266511Z",
     "iopub.status.busy": "2025-10-14T22:37:08.266343Z",
     "iopub.status.idle": "2025-10-14T22:37:09.609599Z",
     "shell.execute_reply": "2025-10-14T22:37:09.609072Z",
     "shell.execute_reply.started": "2025-10-14T22:37:08.266495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name FROM students WHERE YEAR > 2020\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import torch, os\n",
    "\n",
    "BASE = \"google/flan-t5-base\"\n",
    "ADAPTERS = os.path.join(os.path.expanduser(\"~\"), \"text2sql_outputs\", \"lora_adapters\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTERS)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE).to(\"cuda\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTERS).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def generate_sql(question, schema_text, max_src_len=512, max_new=160):\n",
    "    prompt = f\"translate to sql: {question}\\n{schema_text}\"\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", max_length=max_src_len, truncation=True).to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        out_ids = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.2,\n",
    "            length_penalty=0.8,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "schema_text = \"\"\"DB: university\n",
    "Tables:\n",
    "- students(id PK, name, year)\n",
    "- enrollments(id PK, student_id FK->students.id, course_id FK->courses.id)\n",
    "FKs: enrollments.student_id->students.id; enrollments.course_id->courses.id\n",
    "\"\"\"\n",
    "print(generate_sql(\"list names of students after 2020\", schema_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef5c1e-ae3f-47f9-b007-f080cfa3690c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
